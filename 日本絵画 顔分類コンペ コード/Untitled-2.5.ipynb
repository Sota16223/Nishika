{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: torch==2.6.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\testb\\lib\\site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込みと前処理\n",
    "csv_path = 'C:\\\\Users\\\\admin\\\\Documents\\\\testB\\\\data\\\\train.csv'\n",
    "data_dir = 'C:\\\\Users\\\\admin\\\\Documents\\\\testB\\\\data\\\\train\\\\train'\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df_filtered = df[df['gender_status'] != 5]\n",
    "\n",
    "def center_crop_and_resize(img, size):\n",
    "    h, w, _ = img.shape\n",
    "    min_dim = min(h, w)\n",
    "    start_x = (w - min_dim) // 2\n",
    "    start_y = (h - min_dim) // 2\n",
    "    cropped_img = img[start_y:start_y + min_dim, start_x:start_x + min_dim]\n",
    "    resized_img = cv2.resize(cropped_img, (size, size))\n",
    "    return resized_img\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for index, row in df_filtered.iterrows():\n",
    "    img_path = os.path.join(data_dir, row['image'])\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = center_crop_and_resize(img, 224)\n",
    "        x_data.append(img)\n",
    "        y_data.append(row['gender_status'])\n",
    "    else:\n",
    "        print(f\"画像が見つからないか、読み込めませんでした: {img_path}\")\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カテゴリ 1 の画像枚数: 1294\n",
      "カテゴリ 0 の画像枚数: 1210\n",
      "カテゴリ 4 の画像枚数: 834\n",
      "カテゴリ 2 の画像枚数: 375\n",
      "カテゴリ 3 の画像枚数: 371\n",
      "カテゴリ 7 の画像枚数: 100\n",
      "カテゴリ 6 の画像枚数: 53\n",
      "カテゴリ 5 の画像枚数: 1\n"
     ]
    }
   ],
   "source": [
    "# 各カテゴリの画像の枚数を数える\n",
    "category_counts = df['gender_status'].value_counts()\n",
    "\n",
    "# 結果を表示\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"カテゴリ {category} の画像枚数: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Class Weights Dictionary: {0: np.float64(0.5002361275088548), 1: np.float64(0.4677633031574299), 2: np.float64(1.614095238095238), 3: np.float64(1.6314978821717365), 4: np.float64(0.7257622473449812), 5: np.float64(11.420485175202156), 6: np.float64(6.0528571428571425)}\n"
     ]
    }
   ],
   "source": [
    "# クラスの不均等調整（クラス5の重みを無視）\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_data),  # クラス5も含むクラス全体を指定\n",
    "    y=y_data\n",
    ")\n",
    "\n",
    "# クラス5の重みをゼロに設定\n",
    "class_weights = np.array(class_weights)  # Numpy配列に変換\n",
    "#class_weights[5] = 0  # クラス5の重みをゼロに設定\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Tensor形式に変換してPyTorchに渡す\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "weights_tensor = weights_tensor.to(device)\n",
    "# 損失関数の設定\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "print(\"Class Weights Dictionary:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainデータとtestデータをシャッフルしてクラスの均等性を揃える\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.3, random_state=42,stratify=y_data)\n",
    "\n",
    "# PyTorch Dataset作成\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.x_data[idx]\n",
    "        label = self.y_data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ拡張と変換\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(360),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(x_train, y_train, transform=transform)\n",
    "val_dataset = CustomDataset(x_val, y_val, transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model = models.resnet50(pretrained=True)\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "base_model.fc = nn.Sequential(\n",
    "    nn.Linear(base_model.fc.in_features, 256),\n",
    "    #nn.ReLU(),\n",
    "    #nn.Dropout(0.25),\n",
    "    nn.Linear(256, 7),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights)).to(device))\n",
    "optimizer = optim.Adam(base_model.fc.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# トレーニングループの実行\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m train_model(base_model, train_loader, val_loader, criterion, optimizer, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1297\u001b[0m         target,\n\u001b[0;32m   1298\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1299\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1300\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1301\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1302\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3495\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3496\u001b[0m     target,\n\u001b[0;32m   3497\u001b[0m     weight,\n\u001b[0;32m   3498\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3499\u001b[0m     ignore_index,\n\u001b[0;32m   3500\u001b[0m     label_smoothing,\n\u001b[0;32m   3501\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "# トレーニングループの定義\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        # モデルをトレーニングモードに設定\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        # トレーニングデータを処理\n",
    "        for images, labels in train_loader:\n",
    "            images = images.float()\n",
    "            labels = labels.float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # モデルを検証モードに設定\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# トレーニングループの実行\n",
    "train_model(base_model, train_loader, val_loader, criterion, optimizer, device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# トレーニングループの実行\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m train_model(base_model, train_loader, val_loader, criterion, optimizer, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# `torch.float32` に変換\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1297\u001b[0m         target,\n\u001b[0;32m   1298\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1299\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1300\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1301\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1302\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\testB\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3495\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3496\u001b[0m     target,\n\u001b[0;32m   3497\u001b[0m     weight,\n\u001b[0;32m   3498\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3499\u001b[0m     ignore_index,\n\u001b[0;32m   3500\u001b[0m     label_smoothing,\n\u001b[0;32m   3501\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# トレーニングループの定義\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        # モデルをトレーニングモードに設定\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        # トレーニングデータを処理\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(torch.long)  # 交差エントロピー損失関数は `Long` 型を期待\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images).to(torch.float32)  # `torch.float32` に変換\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)  # 修正\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # モデルを検証モードに設定\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)  # デバイスに送る\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)  # 修正\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# トレーニングループの実行\n",
    "train_model(base_model, train_loader, val_loader, criterion, optimizer, device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エポック数\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# 損失のプロット\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, 'ro-', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'bo-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 精度のプロット\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_accuracies, 'ro-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracies, 'bo-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# プロットの表示\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータのディレクトリパスを設定\n",
    "test_data_dir = 'C:\\\\Users\\\\adomin\\\\Documents\\\\testB\\\\data\\\\test\\\\test'\n",
    "\n",
    "# テストデータの画像を読み込み、前処理\n",
    "x_test = []\n",
    "image_names = []\n",
    "\n",
    "for img_name in os.listdir(test_data_dir):\n",
    "    img_path = os.path.join(test_data_dir, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        x_test.append(img)\n",
    "        image_names.append(img_name)\n",
    "    else:\n",
    "        print(f\"画像が見つからないか、読み込めませんでした: {img_path}\")\n",
    "\n",
    "# PyTorch Tensorへの変換\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "x_test_tensor = torch.stack([transform(img) for img in x_test])  # テストデータをTensor形式に変換\n",
    "x_test_tensor = x_test_tensor.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# 予測を行う\n",
    "base_model.eval()  # モデルを評価モードに切り替え\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():  # 勾配計算を無効化\n",
    "    for img_tensor in x_test_tensor:\n",
    "        img_tensor = img_tensor.unsqueeze(0)  # バッチサイズ1に対応させる\n",
    "        output = base_model(img_tensor)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# 結果をDataFrameにまとめる\n",
    "results_df = pd.DataFrame({'image': image_names, 'gender_status': predictions})\n",
    "\n",
    "# 結果をCSVファイルに保存\n",
    "results_csv_path = 'C:\\\\Users\\\\adomin\\\\Documents\\\\testB\\\\data\\\\predictions8.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "print(f\"予測結果が {results_csv_path} に保存されました。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
